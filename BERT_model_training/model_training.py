# -*- coding: utf-8 -*-
"""sentiment_model_training

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11bXqk5im02H1B8lFFTahJQUv54ptAOC7

# Project Setup & Installing Dependencies
"""

# !pip install pandas transformers torch scikit-learn nltk wordcloud

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import json
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer,BertModel
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import os
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader

df = pd.read_csv('/content/drive/MyDrive/hospital_sentiment_project/medical_review_data.csv')

df.head()

df.shape

df.info()

# checking missing values
df.isnull().sum()

# dropping duplicates
df.drop_duplicates(inplace=True)

"""# Preprocesing and Visualization"""

# clean the feedback column removing special char like (#!:)

def clean_feedback(text):
    text = str(text).lower()
    text = re.sub(r'[^\w\s\'-]', '', text)
    return text.strip()

df['cleaned_feedback'] = df['Feedback'].apply(clean_feedback)

# split comma-separated aspects
df['Aspect'] = df['Aspect'].str.split(',')
df = df.explode('Aspect').reset_index(drop=True)
df['Aspect'] = df['Aspect'].str.strip()

# plot sentiment distribution
sns.countplot(x='Sentiment', data=df)
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment (0=Negative, 1=Positive)')
plt.ylabel('Count')
plt.show()

#plot aspect distribution
plt.figure(figsize=(10, 6))
sns.countplot(x='Aspect', data=df, palette='deep')
plt.title('Aspect Distribution')
plt.xlabel('Aspect')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

# aspect wise sentiment distribution
pivot_df = df.groupby(['Aspect', 'Sentiment']).size().unstack(fill_value=0)
pivot_df.plot(kind='bar', stacked=True, figsize=(10, 6))
plt.title('Sentiment by Aspect')
plt.xlabel('Aspect')
plt.ylabel('Count')
plt.legend(['Negative', 'Positive'])
plt.xticks(rotation=45)
plt.show()

# Combine cleaned feedback into a single string
feedback_text = ' '.join(df['cleaned_feedback'].dropna())

# display word cloud of feedback
wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(feedback_text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Hospital Feedback')
plt.show()

# label encoding of aspect (target var)

aspect_encoder = LabelEncoder()
sentiment_encoder = LabelEncoder()

df['aspect_encoded'] = aspect_encoder.fit_transform(df['Aspect'])

# saving the encoded aspect

aspect_mapping = {str(k): int(v) for k, v in zip(aspect_encoder.classes_, aspect_encoder.transform(aspect_encoder.classes_))}
with open('/content/drive/MyDrive/hospital_sentiment_project/aspect_mapping.json', 'w') as f:
    json.dump(aspect_mapping, f)

df.head()

"""# Splitting the dataset"""

X = df['cleaned_feedback']
y_sentiment = df['Sentiment']
y_aspect = df['aspect_encoded']

stratify_labels = (y_sentiment.astype(str) + "_" + y_aspect.astype(str))

X_train, X_test, y_sentiment_train, y_sentiment_test, y_aspect_train, y_aspect_test = train_test_split(X, y_sentiment, y_aspect, test_size=0.2, stratify=stratify_labels, random_state=42)

train_df = df.iloc[X_train.index].copy()
train_df['Sentiment'] = y_sentiment_train
train_df['aspect_encoded'] = y_aspect_train
test_df = df.iloc[X_test.index].copy()
test_df['Sentiment'] = y_sentiment_test
test_df['aspect_encoded'] = y_aspect_test

"""# Tokenize the data for BERT training"""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# tokenize train data
train_encodings = tokenizer(
     train_df['cleaned_feedback'].tolist(),
     padding='max_length',
     max_length=128,
     truncation=True,
     return_tensors='pt'
    )
# tokenize test data
test_encodings = tokenizer(
    test_df['cleaned_feedback'].tolist(),
    padding='max_length',
    max_length=128,
    truncation=True,
    return_tensors='pt'
   )

# enable accurate CUDA error messages
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# dataset definition
class MultiTaskDataset(Dataset):
    def __init__(self, encodings, sentiments, aspects):
        self.encodings = encodings
        self.sentiments = sentiments
        self.aspects = aspects

    def __len__(self):
        return len(self.sentiments)

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['sentiment'] = torch.tensor(self.sentiments[idx], dtype=torch.long)
        item['aspect'] = torch.tensor(self.aspects[idx], dtype=torch.long)
        return item

# create datasets
train_dataset = MultiTaskDataset(train_encodings, y_sentiment_train.values, y_aspect_train.values)
test_dataset = MultiTaskDataset(test_encodings, y_sentiment_test.values, y_aspect_test.values)

train_dataset = MultiTaskDataset(train_encodings, y_sentiment_train.values, y_aspect_train.values)
test_dataset = MultiTaskDataset(test_encodings, y_sentiment_test.values, y_aspect_test.values)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

# creating model training class
class MultiTaskBert(nn.Module):
    def __init__(self):
        super(MultiTaskBert, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.sentiment_dropout = nn.Dropout(0.1)
        self.sentiment_classifier = nn.Linear(768, 2)
        self.aspect_dropout = nn.Dropout(0.1)
        self.aspect_classifier = nn.Linear(768, 7)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled = outputs.pooler_output
        sentiment_logits = self.sentiment_classifier(self.sentiment_dropout(pooled))
        aspect_logits = self.aspect_classifier(self.aspect_dropout(pooled))
        return sentiment_logits, aspect_logits

model = MultiTaskBert().to(device)

# class imbalance handling
sentiment_counts = np.bincount(y_sentiment_train)
weight_0 = len(y_sentiment_train) / (2.0 * sentiment_counts[0])
weight_1 = len(y_sentiment_train) / (2.0 * sentiment_counts[1])
sentiment_weights = torch.tensor([weight_0, weight_1], dtype=torch.float).to(device)

# losses
sentiment_criterion = nn.CrossEntropyLoss(weight=sentiment_weights)
aspect_criterion = nn.CrossEntropyLoss()

# optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)

"""# Training Model"""

from tqdm import tqdm

epochs = 3

for epoch in range(epochs):
    model.train()
    total_loss = 0

    loop = tqdm(train_loader, leave=True, desc=f"Epoch {epoch+1}")

    for batch in loop:
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        sentiment_labels = batch['sentiment'].to(device)
        aspect_labels = batch['aspect'].to(device)

        sentiment_logits, aspect_logits = model(input_ids, attention_mask)

        sentiment_loss = sentiment_criterion(sentiment_logits, sentiment_labels)
        aspect_loss = aspect_criterion(aspect_logits, aspect_labels)

        loss = sentiment_loss + aspect_loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        loop.set_postfix(loss=loss.item())

    avg_loss = total_loss / len(train_loader)
    print(f"\n Epoch {epoch+1} Completed | Average Loss: {avg_loss:.4f}\n")

print(" Training Finished!")

"""# Saving the model"""

torch.save(model.state_dict(), '/content/drive/MyDrive/hospital_sentiment_project/multitask_bert_model.pth')

"""# Model Evaluation"""

def evaluate_overall(model, test_loader, device):
    model.eval()
    all_sentiment_preds = []
    all_sentiment_labels = []
    all_aspect_preds = []
    all_aspect_labels = []

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            sentiment_labels = batch['sentiment'].to(device)
            aspect_labels = batch['aspect'].to(device)

            # Forward pass
            sentiment_logits, aspect_logits = model(input_ids=input_ids, attention_mask=attention_mask)
            sentiment_preds = torch.argmax(sentiment_logits, dim=1)
            aspect_preds = torch.argmax(aspect_logits, dim=1)

            # Store predictions and labels
            all_sentiment_preds.extend(sentiment_preds.cpu().numpy())
            all_sentiment_labels.extend(sentiment_labels.cpu().numpy())
            all_aspect_preds.extend(aspect_preds.cpu().numpy())
            all_aspect_labels.extend(aspect_labels.cpu().numpy())

    # --- Sentiment Metrics ---
    sentiment_acc = accuracy_score(all_sentiment_labels, all_sentiment_preds)
    sentiment_prec = precision_score(all_sentiment_labels, all_sentiment_preds, average='macro')
    sentiment_rec = recall_score(all_sentiment_labels, all_sentiment_preds, average='macro')
    sentiment_f1 = f1_score(all_sentiment_labels, all_sentiment_preds, average='macro')

    # --- Aspect Metrics ---
    aspect_acc = accuracy_score(all_aspect_labels, all_aspect_preds)
    aspect_prec = precision_score(all_aspect_labels, all_aspect_preds, average='macro')
    aspect_rec = recall_score(all_aspect_labels, all_aspect_preds, average='macro')
    aspect_f1 = f1_score(all_aspect_labels, all_aspect_preds, average='macro')

    # --- Print Results ---
    print("\nSentiment Metrics:")
    print(f"Accuracy : {sentiment_acc:.2f}")
    print(f"Precision: {sentiment_prec * 100:.1f}")
    print(f"Recall   : {sentiment_rec * 100:.1f}")
    print(f"F1 Score : {sentiment_f1 * 100:.1f}")

    print("\nAspect Metrics:")
    print(f"Accuracy : {aspect_acc:.2f}")
    print(f"Precision: {aspect_prec * 100:.1f}")
    print(f"Recall   : {aspect_rec * 100:.1f}")
    print(f"F1 Score : {aspect_f1 * 100:.1f}")

# run evaluation
evaluate_overall (model, test_loader, device)

"""# Prediction Function"""

# load tokenizer and model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/hospital_sentiment_project/bert_tokenizer')
model = MultiTaskBert()
model.load_state_dict(torch.load('/content/drive/MyDrive/hospital_sentiment_project/multitask_bert_model.pth'))
model.to(device)
model.eval()

# prediction function
def predict_custom_input(text):
    encodings = tokenizer(text, padding='max_length', max_length=128, truncation=True, return_tensors='pt')
    input_ids = encodings['input_ids'].to(device)
    attention_mask = encodings['attention_mask'].to(device)

    # get predictions
    with torch.no_grad():
        sentiment_logits, aspect_logits = model(input_ids, attention_mask)
        sentiment_pred = torch.argmax(sentiment_logits, dim=1).item()
        aspect_pred = torch.argmax(aspect_logits, dim=1).item()

    # map predictions to labels
    sentiment_map = {0: "Negative", 1: "Positive"}
    aspect_map = {0: "cleanliness", 1: "cost", 2: "emergency", 3: "general", 4: "staff", 5: "treatment", 6: "waiting_time"}

    return sentiment_map[sentiment_pred], aspect_map[aspect_pred]

# test with custom input
custom_input = "Average service and some staffs' behaviour is very bad specially nurses. "
sentiment, aspect = predict_custom_input(custom_input)
print(f"Custom Input: {custom_input}")
print(f"Predicted Sentiment: {sentiment}")
print(f"Predicted Aspect: {aspect}")